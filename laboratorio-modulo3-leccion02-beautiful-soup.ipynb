{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://github.com/Hack-io-Data/Imagenes/blob/main/01-LogosHackio/logo_naranja@4x.png?raw=true\" alt=\"esquema\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexto\n",
    "\n",
    "**Descripción:**\n",
    "\n",
    "SetMagic Productions es una empresa especializada en la provisión de servicios integrales para la realización de rodajes cinematográficos y audiovisuales. Nos dedicamos a facilitar tanto el atrezzo necesario para las producciones como los lugares idóneos para llevar a cabo los rodajes, ya sea en entornos al aire libre o en interiores.\n",
    "\n",
    "**Servicios Ofrecidos:**\n",
    "\n",
    "- **Atrezzo Creativo:** Contamos con un extenso catálogo de atrezzo que abarca desde accesorios hasta muebles y objetos temáticos para ambientar cualquier tipo de  escena.\n",
    "\n",
    "- **Locaciones Únicas:** Nuestra empresa ofrece una amplia selección de locaciones, que incluyen desde escenarios naturales como playas, bosques y montañas, hasta espacios interiores como estudios, casas históricas y edificios emblemáticos.\n",
    "- **Servicios de Producción:** Además de proporcionar atrezzo y locaciones, también ofrecemos servicios de producción audiovisual, incluyendo equipos de filmación, personal técnico y servicios de postproducción.\n",
    "\n",
    "**Herramientas y Tecnologías:**\n",
    "\n",
    "Para recopilar información sobre nuevas locaciones y tendencias en atrezzo, utilizamos herramientas de web scraping como Beautiful Soup y Selenium para extraer datos de sitios web relevantes y redes sociales especializadas en cine y producción audiovisual. También integramos APIs de plataformas de alquiler de locaciones y bases de datos de atrezzo para acceder a información actualizada y detallada.\n",
    "\n",
    "**Almacenamiento de Datos:** (A trabajar la próxima semana)\n",
    "\n",
    "La información recopilada mediante web scraping y APIs se almacenará tanto en una base de datos relacional SQL como en una base de datos no relacional MongoDB . Estas base de datos nos permite organizar eficientemente la información sobre locaciones, atrezzo, clientes y proyectos en curso, facilitando su acceso y gestión.\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Nuestro objetivo principal es proporcionar a nuestros clientes una experiencia fluida y personalizada en la búsqueda y selección de locaciones y atrezzo para sus proyectos audiovisuales. Utilizando tecnologías avanzadas y una amplia red de contactos en la industria, nos esforzamos por ofrecer soluciones creativas y de alta calidad que satisfagan las necesidades específicas de cada producción.\n",
    "\n",
    "\n",
    "## Lab: Extracción de Información con Beautiful Soup\n",
    "\n",
    "En este laboratorio seguirás enriqueciendo la base de datos para ofrecer a tus clientes un servicio más completo y personalizado. Para lograr esto, extraerás información valiosa sobre objetos de atrezzo de una web especializada. Utilizarás técnicas de web scraping con la biblioteca **Beautiful Soup** para obtener y organizar los datos de manera eficiente.\n",
    "\n",
    "Trabajarás con la siguiente URL: [Atrezzo Vázquez](https://atrezzovazquez.es/shop.php?search_type=-1&search_terms=&limit=48&page=1). Esta página contiene una gran cantidad de objetos de atrezzo que pueden ser de interés para un proyecto de rodaje. Tu tarea será extraer información relevante de los productos listados en las primeras 100 páginas.\n",
    "\n",
    "1. **Navegación y Extracción de Múltiples Páginas**:\n",
    "\n",
    "   - La página web contiene varios elementos distribuidos en distintas páginas. Tu objetivo será iterar a través de las 100 primeras páginas y extraer la información deseada.\n",
    "\n",
    "   - Debes asegurarte de que tu código sea capaz de navegar automáticamente por estas páginas y extraer datos de manera continua.\n",
    "\n",
    "2. **Verificación del Código de Estado de la Respuesta**:\n",
    "\n",
    "   - Antes de extraer cualquier información, es fundamental verificar que la solicitud a la página web ha sido exitosa. Un código de estado 200 indica que la página se ha cargado correctamente.\n",
    "\n",
    "   - Si el código no es 200, debes imprimir un mensaje de error y detener la ejecución de la extracción para evitar problemas posteriores.\n",
    "\n",
    "3. **Extracción de Información Específica**:\n",
    "\n",
    "   De cada página, deberás extraer los siguientes detalles de los objetos de atrezzo:\n",
    "\n",
    "   - **Nombre del Objeto**: El nombre o identificador del objeto.\n",
    "\n",
    "   - **Categoría**: La categoría en la que se clasifica el objeto (ej.: mobiliario, decorado, utilería, etc.).\n",
    "\n",
    "   - **Sección**: La sección específica dentro de la categoría.\n",
    "\n",
    "   - **Descripción**: Una breve descripción del objeto que puede incluir detalles sobre su estilo, material o uso.\n",
    "\n",
    "   - **Dimensiones**: El tamaño del objeto en formato (largo x ancho x alto).\n",
    "\n",
    "   - **Enlace a la Imagen**: El link a la imagen del objeto, útil para tener una vista previa visual.\n",
    "\n",
    "4. **Organización de los Datos en un Diccionario**:\n",
    "\n",
    "   Una vez extraída la información, deberás organizarla en un diccionario con las siguientes claves:\n",
    "\n",
    "   - `\"nombre\"`: Nombres del objeto.\n",
    "\n",
    "   - `\"categoria\"`: Categoría a la que pertenece el objeto.\n",
    "\n",
    "   - `\"seccion\"`: Sección específica dentro de la categoría.\n",
    "\n",
    "   - `\"descripcion\"`: Breve descripción del objeto.\n",
    "\n",
    "   - `\"dimensiones\"`: Dimensiones del objeto en el formato adecuado.\n",
    "\n",
    "   - `\"imagen\"`: URL de la imagen del objeto.\n",
    "\n",
    "5. **Almacenamiento de la Información en un DataFrame**:\n",
    "\n",
    "   Una vez que tengas toda la información organizada en diccionarios, el siguiente paso es convertirla en un DataFrame de Pandas. Este DataFrame te permitirá manipular y analizar los datos con facilidad. Tu DataFrame debería verse similar al ejemplo proporcionado, donde cada fila representa un objeto diferente y cada columna contiene la información extraída:\n",
    "\n",
    "![Dataframe](https://github.com/Hack-io-Data/Imagenes/blob/main/02-Imagenes/BS/df_atrezzo.png?raw=true)\n",
    "\n",
    "\n",
    "6.  Consideraciones Adicionales:\n",
    "\n",
    "- Asegúrate de manejar posibles errores o excepciones durante el scraping, como tiempos de espera agotados, páginas inaccesibles o datos faltantes.\n",
    "\n",
    "- Recuerda que el scraping debe hacerse de manera respetuosa, evitando sobrecargar el servidor de la página web (puedes usar pausas entre solicitudes).\n",
    "\n",
    "- Finalmente, asegúrate de almacenar el DataFrame en un archivo CSV para que puedas reutilizar esta información en análisis futuros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías de extracción de datos\n",
    "# -----------------------------------------------------------------------\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Tratamiento de datos\n",
    "# -----------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# Creamos la sopa (el html de la web)\n",
    "\n",
    "url_atrezzo = \"https://atrezzovazquez.es/shop.php?search_type=-1&search_terms=&limit=48&page=1\"\n",
    "\n",
    "res_atrezzo = requests.get(url_atrezzo)\n",
    "print(res_atrezzo.status_code)\n",
    "\n",
    "sopa_atrezzo = BeautifulSoup(res_atrezzo.content, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_prod_cat = sopa_atrezzo.findAll(\"div\", {\"class\":\"product-slide-entry shift-image\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres_productos = []\n",
    "categorias_productos = []\n",
    "\n",
    "for producto in lista_prod_cat:\n",
    "    try:\n",
    "        nombre = producto.findAll(\"a\", {\"class\": \"title\"})[0].getText()\n",
    "        nombres_productos.append(nombre.getText())\n",
    "\n",
    "    except:\n",
    "        nombres_productos.append(np.nan)\n",
    "\n",
    "    try:\n",
    "        categoria = producto.findAll(\"a\", {\"class\": \"tag\"})[0].getText()\n",
    "        categorias_productos.apprend(categoria.getText())\n",
    "        \n",
    "    except:\n",
    "        categorias_productos.append(np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categorias_productos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nombres_productos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_secciones = sopa_atrezzo.findAll(\"div\", {\"class\":\"cat-sec-box\"})\n",
    "\n",
    "secciones_productos = []\n",
    "\n",
    "for seccion in lista_secciones:\n",
    "    if len(seccion.getText()) > 1: # hay resultados\n",
    "        secciones_productos.append(seccion.getText())\n",
    "\n",
    "    else:\n",
    "        secciones_productos.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(secciones_productos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_descripciones = sopa_atrezzo.findAll(\"p\")\n",
    "descripciones_productos = []\n",
    "\n",
    "for descripcion in lista_descripciones:\n",
    "    descripciones_productos.append(descripcion.getText())\n",
    "\n",
    "descripciones_productos = descripciones_productos[:48]\n",
    "# Puedo quedarme con los primeros 48 porque sé que los últimos siempre están vacíos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(descripciones_productos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_dimensiones = sopa_atrezzo.findAll(\"div\", {\"class\": \"price\"})\n",
    "dimensiones_productos = []\n",
    "\n",
    "for dimension in lista_dimensiones:\n",
    "    dimensiones_productos.append(dimension.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dimensiones_productos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_imagenes = sopa_atrezzo.findAll(\"div\", {\"class\": \"product-image\"})\n",
    "\n",
    "imagenes_productos = []\n",
    "\n",
    "for imagen in lista_imagenes:\n",
    "    try:\n",
    "        link = imagen.findAll(\"img\")[0].get(\"srtc\")\n",
    "        imagenes_productos.append(link)\n",
    "\n",
    "    except:\n",
    "        imagenes_productos.append(np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imagenes_productos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'type'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 72\u001b[0m\n\u001b[1;32m     67\u001b[0m         imagenes_productos\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m     70\u001b[0m df_atrezzo \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnombre\u001b[39m\u001b[38;5;124m\"\u001b[39m:nombres_productos,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msección\u001b[39m\u001b[38;5;124m\"\u001b[39m:categorias_productos,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescripción\u001b[39m\u001b[38;5;124m\"\u001b[39m:descripciones_productos,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimensiones\u001b[39m\u001b[38;5;124m\"\u001b[39m:dimensiones_productos,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m:imagenes_productos})\n\u001b[0;32m---> 72\u001b[0m df_completo \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_atrezzo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_completo\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/concat.py:448\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ndims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m sample, objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sample_object(objs, ndims, keys, names, levels)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# Standardize axis parameter to int\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/concat.py:489\u001b[0m, in \u001b[0;36m_Concatenator._get_ndims\u001b[0;34m(self, objs)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (ABCSeries, ABCDataFrame)):\n\u001b[1;32m    485\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot concatenate object of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly Series and DataFrame objs are valid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m         )\n\u001b[0;32m--> 489\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    491\u001b[0m     ndims\u001b[38;5;241m.\u001b[39madd(obj\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ndims\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'type'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "df_completo = pd.DataFrame\n",
    "\n",
    "\n",
    "for page in tqdm(range(1,5)):\n",
    "    url_atrezzo = \"https://atrezzovazquez.es/shop.php?search_type=-1&search_terms=&limit=48&page={page}\"\n",
    "    res_atrezzo = requests.get(url_atrezzo)\n",
    "\n",
    "    if (res_atrezzo.status_code) != 200:\n",
    "        print(\"Se ha producido un error\")\n",
    "        break\n",
    "    \n",
    "    print(f\"Status code: {res_atrezzo.status_code}\")\n",
    "    sopa_atrezzo = BeautifulSoup(res_atrezzo.content, \"html.parser\")\n",
    "\n",
    "    # nombre y cat\n",
    "    nombres_productos = []\n",
    "    categorias_productos = []\n",
    "\n",
    "    for producto in lista_prod_cat:\n",
    "        try:\n",
    "            nombre = producto.findAll(\"a\", {\"class\": \"title\"})[0].getText()\n",
    "            nombres_productos.append(nombre.getText())\n",
    "\n",
    "        except:\n",
    "            nombres_productos.append(np.nan)\n",
    "\n",
    "        try:\n",
    "            categoria = producto.findAll(\"a\", {\"class\": \"tag\"})[0].getText()\n",
    "            categorias_productos.apprend(categoria.getText())\n",
    "            \n",
    "        except:\n",
    "            categorias_productos.append(np.nan)\n",
    "\n",
    "    #sección\n",
    "    lista_secciones = sopa_atrezzo.findAll(\"div\", {\"class\":\"cat-sec-box\"})\n",
    "\n",
    "    secciones_productos = []\n",
    "\n",
    "    for seccion in lista_secciones:\n",
    "        if len(seccion.getText()) > 1: # hay resultados\n",
    "            secciones_productos.append(seccion.getText())\n",
    "\n",
    "        else:\n",
    "            secciones_productos.append(np.nan)\n",
    "\n",
    "    #descripción\n",
    "    lista_descripciones = sopa_atrezzo.findAll(\"p\")\n",
    "    descripciones_productos = []\n",
    "\n",
    "    for descripcion in lista_descripciones:\n",
    "        descripciones_productos.append(descripcion.getText())\n",
    "\n",
    "    descripciones_productos = descripciones_productos[:48]\n",
    "    # Puedo quedarme con los primeros 48 porque sé que los últimos siempre están vacíos\n",
    "\n",
    "    # link imagen\n",
    "    lista_imagenes = sopa_atrezzo.findAll(\"div\", {\"class\": \"product-image\"})\n",
    "\n",
    "    imagenes_productos = []\n",
    "\n",
    "    for imagen in lista_imagenes:\n",
    "        try:\n",
    "            link = imagen.findAll(\"img\")[0].get(\"srtc\")\n",
    "            imagenes_productos.append(link)\n",
    "\n",
    "        except:\n",
    "            imagenes_productos.append(np.nan)\n",
    "\n",
    "\n",
    "    df_atrezzo = pd.DataFrame({\"nombre\":nombres_productos,\"sección\":categorias_productos,\"descripción\":descripciones_productos,\"dimensiones\":dimensiones_productos,\"url\":imagenes_productos})\n",
    "\n",
    "    df_completo = pd.concat([df_atrezzo, df_completo])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
